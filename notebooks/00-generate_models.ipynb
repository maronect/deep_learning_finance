{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5625769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust path to find 'src'\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.data.loader import load_prices, compute_returns\n",
    "from src.models.lr import expected_return_from_predictions, evaluate_prediction_series\n",
    "from outputs.charts.markowitz_plot import compare_time_series, compare_frontiers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.data.loader import load_prices\n",
    "from src.models.lr import predict_daily_series_lr\n",
    "from src.models.rnn import predict_daily_series_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320bd508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/deep_learning_finance/src/data/loader.py:9: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(tickers, start=start, end=end)['Close']\n",
      "[*********************100%***********************]  9 of 9 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LR...\n",
      "Starting LR Walk-Forward (Train Window: 252, Refit: 21)...\n",
      "LR Saved.\n",
      "Running RNN...\n",
      "Training LSTM on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3. Run RNN/LSTM\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning RNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m rnn_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_daily_series_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m rnn_preds\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/models/rnn/pred_daily_series_rnn.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN Saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/deep_learning_finance/src/models/rnn.py:77\u001b[0m, in \u001b[0;36mpredict_daily_series_rnn\u001b[0;34m(prices, seq_len, epochs, hidden_dim)\u001b[0m\n\u001b[1;32m     75\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     76\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(model(xb), yb)\n\u001b[0;32m---> 77\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Predict on Test (Walk-Forward using Actual History)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Setup\n",
    "tickers = [\"PETR4.SA\", \"VALE3.SA\", \"ITUB4.SA\", \"BBDC4.SA\", \"BBAS3.SA\", \"ABEV3.SA\", \"WEGE3.SA\", \"B3SA3.SA\", \"GGBR4.SA\"]\n",
    "prices = load_prices(tickers, start=\"2017-01-01\", end=\"2025-06-01\")\n",
    "\n",
    "# 2. Run Linear Regression\n",
    "print(\"Running LR...\")\n",
    "lr_preds = predict_daily_series_lr(prices, window_features=[5, 21], training_window=252)\n",
    "lr_preds.to_csv(\"../outputs/models/lr/pred_daily_series_lr.csv\")\n",
    "print(\"LR Saved.\")\n",
    "\n",
    "# 3. Run RNN/LSTM\n",
    "print(\"Running RNN...\")\n",
    "rnn_preds = predict_daily_series_rnn(prices, seq_len=20, epochs=30)\n",
    "rnn_preds.to_csv(\"../outputs/models/rnn/pred_daily_series_rnn.csv\")\n",
    "print(\"RNN Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Setup Data\n",
    "tickers = [\"PETR4.SA\", \"VALE3.SA\", \"ITUB4.SA\", \"BBDC4.SA\", \"BBAS3.SA\", \"ABEV3.SA\", \"WEGE3.SA\", \"B3SA3.SA\", \"GGBR4.SA\"]\n",
    "prices = load_prices(tickers, start=\"2017-01-01\", end=\"2025-06-01\")\n",
    "returns_daily = compute_returns(prices, freq=\"daily\")\n",
    "\n",
    "# Historical Stats (for Benchmarks)\n",
    "hist_mean_daily = returns_daily.mean()\n",
    "hist_cov_daily = returns_daily.cov()\n",
    "\n",
    "# 2. Load Predictions\n",
    "# Ensure you ran the generation notebook first!\n",
    "try:\n",
    "    pred_lr = pd.read_csv(\"../outputs/models/lr/pred_daily_series_lr.csv\", index_col=0, parse_dates=True)\n",
    "    pred_rnn = pd.read_csv(\"../outputs/models/rnn/pred_daily_series_rnn.csv\", index_col=0, parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Prediction CSVs not found. Run '1_generate_models.ipynb' first.\")\n",
    "    pred_lr = pd.DataFrame()\n",
    "    pred_rnn = pd.DataFrame()\n",
    "\n",
    "# 3. Calculate \"Average\" Expectations for Frontier Plot\n",
    "# (We assume the average prediction represents the asset's 'true' nature for the static plot)\n",
    "mu_lr_daily, _ = expected_return_from_predictions(pred_lr)\n",
    "mu_rnn_daily, _ = expected_return_from_predictions(pred_rnn)\n",
    "\n",
    "# 4. Define Models Configuration\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"Historical Benchmark\",\n",
    "        \"mean_returns\": hist_mean_daily,\n",
    "        \"cov\": hist_cov_daily,\n",
    "        \"is_monthly\": False,\n",
    "        \"color\": \"black\",\n",
    "        \"pred_series\": None # Static\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Linear Regression\",\n",
    "        \"mean_returns\": mu_lr_daily, # For Frontier\n",
    "        \"cov\": hist_cov_daily,       # Using Hist Cov (Standard practice)\n",
    "        \"is_monthly\": False,\n",
    "        \"color\": \"blue\",\n",
    "        \"pred_series\": pred_lr       # For Backtest\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RNN / LSTM\",\n",
    "        \"mean_returns\": mu_rnn_daily,\n",
    "        \"cov\": hist_cov_daily,\n",
    "        \"is_monthly\": False,\n",
    "        \"color\": \"orange\",\n",
    "        \"pred_series\": pred_rnn\n",
    "    }\n",
    "]\n",
    "\n",
    "# 5. Plot Efficient Frontiers (Static Comparison)\n",
    "print(\"--- Efficient Frontiers (Average Predicted Return vs Risk) ---\")\n",
    "compare_frontiers(models_config)\n",
    "\n",
    "# 6. Plot Time Series (Dynamic Backtest)\n",
    "# This uses the 'pred_series' to rebalance monthly\n",
    "print(\"--- Dynamic Backtest (Monthly Rebalancing) ---\")\n",
    "compare_time_series(returns_daily, models_config, target_risk_annual=0.20)\n",
    "\n",
    "# 7. Metrics Table\n",
    "print(\"--- Model Accuracy Metrics ---\")\n",
    "metrics_lr = evaluate_prediction_series(returns_daily, pred_lr)\n",
    "metrics_lr['Model'] = 'LR'\n",
    "\n",
    "metrics_rnn = evaluate_prediction_series(returns_daily, pred_rnn)\n",
    "metrics_rnn['Model'] = 'RNN'\n",
    "\n",
    "all_metrics = pd.concat([metrics_lr, metrics_rnn])\n",
    "display(all_metrics.groupby(\"Model\")[['MSE', 'R2', 'Corr']].mean())\n",
    "\n",
    "# Optional: Histogram of Predictions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(pred_lr.values.flatten(), bins=50, alpha=0.5, label='LR Preds', density=True)\n",
    "plt.hist(pred_rnn.values.flatten(), bins=50, alpha=0.5, label='RNN Preds', density=True)\n",
    "plt.hist(returns_daily.values.flatten(), bins=100, alpha=0.3, color='gray', label='Real Returns', density=True, range=(-0.05, 0.05))\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Predicted vs Real Returns\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
